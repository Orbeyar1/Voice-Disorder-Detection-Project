{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/Orbeyar1/Voice-Disorder-Detection-Project/blob/main/Audio_split_via_VAD.ipynb","timestamp":1687972263804}],"private_outputs":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"COXmxAQaWR9U"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount = True)\n","# Change this based on the drive being used\n","# my_voice_data = '/content/drive/MyDrive/Project_B/Voice samples study'\n","my_voice_data = '/content/drive/MyDrive/Study_materials/Voice samples study'"]},{"cell_type":"code","source":["!apt-get install libsox-fmt-all\n","!apt-get install sox\n","!pip install sox"],"metadata":{"id":"Om4nUidC1Ujd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sox"],"metadata":{"id":"VEL0jaBH2A2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sox /content/drive/MyDrive/Study_materials/Voice\\ samples\\ study/Patient-127.wav -c 1 pat_127_sox.wav\n","!sox /content/pat_127_sox.wav -r 16000 /content/pat_127_sox_mono.wav\n","!rm pat_127_sox.wav\n","\n","\n","!sox /content/drive/MyDrive/Study_materials/Voice\\ samples\\ study/Patient\\ 01.wav -c 1 pat_01_sox.wav\n","!sox /content/pat_01_sox.wav -r 16000 /content/pat_01_sox_mono.wav\n","!rm pat_01_sox.wav\n","\n","!sox /content/drive/MyDrive/Study_materials/Voice\\ samples\\ study/Patient\\ 97.wav -c 1 pat_97_sox.wav\n","!sox /content/pat_97_sox.wav -r 16000 /content/pat_97_sox_mono.wav\n","!rm pat_97_sox.wav\n","\n","!sox /content/drive/MyDrive/Study_materials/Voice\\ samples\\ study/Patient\\ 151.wav -c 1 pat_151_sox.wav\n","!sox /content/pat_151_sox.wav -r 16000 /content/pat_151_sox_mono.wav\n","!rm pat_151_sox.wav"],"metadata":{"id":"FK0Q-Rnx3BfN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sox --i /content/pat_127_sox_mono.wav\n","!sox --i /content/pat_01_sox_mono.wav\n","!sox --i /content/pat_97_sox_mono.wav\n","!sox --i /content/pat_151_sox_mono.wav"],"metadata":{"id":"avwofHNa5Hjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Imports\n","\n","import os\n","import glob\n","import numpy as np\n","import torch\n","import torchaudio\n","import torchaudio.functional as F\n","import torchaudio.transforms as T\n","import matplotlib.pyplot as plt\n","import librosa\n","import soundfile as sf\n","from IPython.display import Audio\n","\n","print(torch.__version__)\n","print(torchaudio.__version__)\n","\n","## Use cuda if available\n","torch.random.manual_seed(0)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(device)"],"metadata":{"id":"3cJx2KuuXK-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define constants\n","wanted_sample_rate = 16000\n","\n","audio_labels = [\"normal pitch E\",\"low pitch E\",\"high pitch E\",\n","                \"normal pitch Ah\",\"low pitch Ah\",\"high pitch Ah\",\n","                \"normal pitch Ooh\",\"low pitch Ooh\",\"high pitch Ooh\",\n","                \"normal pitch Mmmmm\"]\n","\n","def plot_waveform(waveform, sample_rate):\n","    waveform = waveform.numpy()\n","    num_channels, num_frames = waveform.shape\n","    time_axis = torch.arange(0, num_frames) / sample_rate\n","\n","    figure, axes = plt.subplots(num_channels, 1)\n","    if num_channels == 1:\n","        axes = [axes]\n","    for c in range(num_channels):\n","        axes[c].plot(time_axis, waveform[c], linewidth=1)\n","        axes[c].grid(True)\n","        if num_channels > 1:\n","            axes[c].set_ylabel(f\"Channel {c+1}\")\n","    figure.suptitle(\"waveform\")\n","    plt.show(block=False)"],"metadata":{"id":"x89PJopTPGZi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the data using torchaudio\n","(probably not going to work, need to use SoX)"],"metadata":{"id":"P35g9TP83tIB"}},{"cell_type":"code","source":["# ### Load Data ##\n","\n","# # First we count number of .wav files\n","# counter = 0\n","# for filename in os.listdir(my_voice_data):\n","#     if filename.endswith(\"wav\"):\n","#       counter +=1\n","# print(f'We have {counter} .wav files')\n","\n","# # We bucket sort all patients into an array of mono-channel, 16k S/R audiotorch tensors.\n","# patients_audio_list = [None] * (counter + 10) # We add +2 because there's a mis-lable in filenames\n","# for filename in os.listdir(my_voice_data):\n","#     if filename.endswith(\"wav\"):\n","#       # Extract patient's ID from filename\n","#         _, numberwav = filename.split(\" \")\n","#         number = numberwav.split(\".\")\n","#         patient_id = number[0]\n","#         waveform, sample_rate = torchaudio.load(my_voice_data +'/'+ filename)\n","#         transform = T.Resample(sample_rate, wanted_sample_rate)\n","#         waveform = transform(waveform)\n","#         waveform_mono = torch.mean(waveform, dim=0).unsqueeze(0)\n","#         # if sample_rate != wanted_sample_rate:\n","#         #   waveform = torchaudio.functional.resample(waveform, sample_rate, wanted_sample_rate)\n","#         patients_audio_list[int(patient_id)] = (int(patient_id),waveform_mono,wanted_sample_rate)\n"],"metadata":{"id":"Nbq09vHiX_p1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Google's VAD import, defs and methods:"],"metadata":{"id":"J2t-Nj4e3oL5"}},{"cell_type":"code","source":["!pip install webrtcvad\n","import webrtcvad"],"metadata":{"id":"XzLAQXOqbNDn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import collections\n","import contextlib\n","import sys\n","import wave\n","\n","\n","\n","def read_wave(path):\n","    \"\"\"Reads a .wav file.\n","\n","    Takes the path, and returns (PCM audio data, sample rate).\n","    \"\"\"\n","    with contextlib.closing(wave.open(path, 'rb')) as wf:\n","        num_channels = wf.getnchannels()\n","        assert num_channels == 1\n","        sample_width = wf.getsampwidth()\n","        assert sample_width == 2\n","        sample_rate = wf.getframerate()\n","        assert sample_rate in (8000, 16000, 32000, 48000)\n","        pcm_data = wf.readframes(wf.getnframes())\n","        return pcm_data, sample_rate\n","\n","\n","def write_wave(path, audio, sample_rate):\n","    \"\"\"Writes a .wav file.\n","\n","    Takes path, PCM audio data, and sample rate.\n","    \"\"\"\n","    with contextlib.closing(wave.open(path, 'wb')) as wf:\n","        wf.setnchannels(1)\n","        wf.setsampwidth(2)\n","        wf.setframerate(sample_rate)\n","        wf.writeframes(audio)\n","\n","\n","\n","class Frame(object):\n","    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n","    def __init__(self, bytes, timestamp, duration):\n","        self.bytes = bytes\n","        self.timestamp = timestamp\n","        self.duration = duration\n","\n","\n","def frame_generator(frame_duration_ms, audio, sample_rate):\n","    \"\"\"Generates audio frames from PCM audio data.\n","\n","    Takes the desired frame duration in milliseconds, the PCM data, and\n","    the sample rate.\n","\n","    Yields Frames of the requested duration.\n","    \"\"\"\n","    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n","    offset = 0\n","    timestamp = 0.0\n","    duration = (float(n) / sample_rate) / 2.0\n","    while offset + n < len(audio):\n","        yield Frame(audio[offset:offset + n], timestamp, duration)\n","        timestamp += duration\n","        offset += n\n","\n","\n","def vad_collector(sample_rate, frame_duration_ms,\n","                  padding_duration_ms, vad, frames):\n","    \"\"\"Filters out non-voiced audio frames.\n","\n","    Given a webrtcvad.Vad and a source of audio frames, yields only\n","    the voiced audio.\n","\n","    Uses a padded, sliding window algorithm over the audio frames.\n","    When more than 90% of the frames in the window are voiced (as\n","    reported by the VAD), the collector triggers and begins yielding\n","    audio frames. Then the collector waits until 90% of the frames in\n","    the window are unvoiced to detrigger.\n","\n","    The window is padded at the front and back to provide a small\n","    amount of silence or the beginnings/endings of speech around the\n","    voiced frames.\n","\n","    Arguments:\n","\n","    sample_rate - The audio sample rate, in Hz.\n","    frame_duration_ms - The frame duration in milliseconds.\n","    padding_duration_ms - The amount to pad the window, in milliseconds.\n","    vad - An instance of webrtcvad.Vad.\n","    frames - a source of audio frames (sequence or generator).\n","\n","    Returns: A generator that yields PCM audio data.\n","    \"\"\"\n","    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n","    # We use a deque for our sliding window/ring buffer.\n","    ring_buffer = collections.deque(maxlen=num_padding_frames)\n","    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n","    # NOTTRIGGERED state.\n","    triggered = False\n","\n","    voiced_frames = []\n","    for frame in frames:\n","        is_speech = vad.is_speech(frame.bytes, sample_rate)\n","\n","        sys.stdout.write('1' if is_speech else '0')\n","        if not triggered:\n","            ring_buffer.append((frame, is_speech))\n","            num_voiced = len([f for f, speech in ring_buffer if speech])\n","            # If we're NOTTRIGGERED and more than 90% of the frames in\n","            # the ring buffer are voiced frames, then enter the\n","            # TRIGGERED state.\n","            if num_voiced > 0.9 * ring_buffer.maxlen:\n","                triggered = True\n","                sys.stdout.write('+(%s)' % (ring_buffer[0][0].timestamp,))\n","                # We want to yield all the audio we see from now until\n","                # we are NOTTRIGGERED, but we have to start with the\n","                # audio that's already in the ring buffer.\n","                for f, s in ring_buffer:\n","                    voiced_frames.append(f)\n","                ring_buffer.clear()\n","        else:\n","            # We're in the TRIGGERED state, so collect the audio data\n","            # and add it to the ring buffer.\n","            voiced_frames.append(frame)\n","            ring_buffer.append((frame, is_speech))\n","            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n","            # If more than 90% of the frames in the ring buffer are\n","            # unvoiced, then enter NOTTRIGGERED and yield whatever\n","            # audio we've collected.\n","            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n","                sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n","                triggered = False\n","                yield b''.join([f.bytes for f in voiced_frames])\n","                ring_buffer.clear()\n","                voiced_frames = []\n","    if triggered:\n","        sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n","    sys.stdout.write('\\n')\n","    # If we have any leftover voiced audio when we run out of input,\n","    # yield it.\n","    if voiced_frames:\n","        yield b''.join([f.bytes for f in voiced_frames])\n"],"metadata":{"id":"hJoDTmmybFRc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["google's VAD usage method:"],"metadata":{"id":"TxGqgJBQ3jFu"}},{"cell_type":"code","source":["def webrtcvad_usage(aggressive=0, audio_path=None,frame_duration=30,padding_duration=300,min_chunk_length = 0.5):\n","    if os.path.isfile('/content/chunk-00.wav'):\n","      print(\"Clearing previous .wav files\")\n","      !rm chunk-??.wav\n","      !rm chunk-???.wav\n","    vad = webrtcvad.Vad(aggressive)\n","    audio, sample_rate  = read_wave(audio_path)\n","    frames = frame_generator(frame_duration, audio, sample_rate)\n","    frames = list(frames)\n","    segments = vad_collector(sample_rate, frame_duration, padding_duration, vad, frames)\n","\n","\n","\n","  #  sample_rate - The audio sample rate, in Hz.\n","  #   frame_duration_ms - The frame duration in milliseconds.\n","  #   padding_duration_ms - The amount to pad the window, in milliseconds.\n","  #   vad - An instance of webrtcvad.Vad.\n","  #   frames - a source of audio frames (sequence or generator).\n","    i = 0\n","    for j,segment in enumerate(segments):\n","      path = 'chunk-%002d.wav' % (i,)\n","      # print(' Writing %s' % (path,))\n","      len_sec = len(segment)/(sample_rate*2)\n","      # if( i < 10):\n","      if((len_sec >= min_chunk_length) ):\n","        write_wave(path, segment, sample_rate)\n","        # print(f'\\n phone Chunk {i} length is {len_sec} seconds')\n","        i = i+1\n","      # else:\n","        # print(f'\\n too short! Chunk {i} length is {len_sec} seconds')\n","    return segments"],"metadata":{"id":"mwN2BIena8cx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install dtw-python"],"metadata":{"id":"I1caVUsyfcGh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Roni's DTW code:"],"metadata":{"id":"f6gsRG3s3ZZf"}},{"cell_type":"code","source":["_author_ = \"Roni Chernyak\"\n","\n","import librosa\n","import numpy as np\n","import os\n","from scipy.spatial.distance import euclidean\n","from dtw import dtw\n","from scipy import stats\n","\n","TEST_PATH = \"/content/chunk-09.wav\"\n","GT_PATH = \"/content/mmm_GTT.wav\"\n","\n","\n","# dynamic programming implementation\n","def DTW(src, trgt):\n","    dtw = np.full((src.shape[1]+1, trgt.shape[1]+1), np.inf, dtype=np.float64)\n","    dtw[0,0] = 0\n","    for i in range(1, src.shape[1]+1):\n","        for j in range(1, trgt.shape[1]+1):\n","            c = euclidean(src[:,i-1], trgt[:,j-1]) # Euclidean distance\n","            #c = np.abs(src[i] - trgt[j])\n","            dtw[i][j] = c + np.min([dtw[i - 1][j],\n","                                    dtw[i][j - 1],\n","                                    dtw[i - 1][j - 1]])\n","    # get the dtw distance\n","    return dtw[-1, -1]\n","\n","def libroa_dtw(path_mfcc1,path_mfcc2):\n","  X = get_mfcc_features(path=path_mfcc1)\n","  Y = get_mfcc_features(path=path_mfcc2)\n","  D, wp = librosa.sequence.dtw(X, Y)\n","  return D[wp[-1,1],wp[-1,0]]\n","\n","def get_mfcc_features(path):\n","    '''\n","    :param path: files path\n","    :return: file's features\n","    '''\n","    y, sr = librosa.load(path, sr=None)\n","    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n","\n","    return stats.zscore(mfcc,axis=1)\n","\n","def compare_mfcc_features(path_mfcc1,path_mfcc2):\n","  mfcc1 = get_mfcc_features(path=path_mfcc1)\n","  mfcc2 = get_mfcc_features(path=path_mfcc2)\n","  return DTW(mfcc1, mfcc2)"],"metadata":{"id":"7CrjmT0EfMuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","def get_num_of_chunk_as_int(chunk):\n","  # receives a chunk's name in str and extracts the num as integer\n","  return (chunk.split(\".\")[0]).split(\"-\")[1]\n","\n","def color_cells(val):\n","    color = 'green' if val == 9 else 'red'\n","    return f'color: {color}'\n","params_list = []\n","\n","df = pd.DataFrame(0,columns = [10,20,30],index =range(100,550,50))\n"],"metadata":{"id":"6F1kA3JOBbGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def file_params_VAD_iterate(aggressive_list=[3],audio_path=None,frame_duration_list=range(10,40,10),padding_duration_list=range(100,550,50)):\n","  print(f'Filename: {audio_path}')\n","  for aggressive_it in aggressive_list: #iterate over list\n","      for frame_duration_it in frame_duration_list: #iterate over list\n","        column_list = []\n","        for padding_duration_it in padding_duration_list: #iterate over list\n","          # Run VAD on said parameters\n","          print(f'Params: aggr: {aggressive_it} frame_ms: {frame_duration_it} padding: {padding_duration_it}')\n","          chunks_web = webrtcvad_usage(aggressive=aggressive_it,audio_path=audio_path,frame_duration=frame_duration_it,padding_duration=padding_duration_it,min_chunk_length = 0.85)\n","          # calculate DTW score between GT and current MFCC features\n","\n","          directory = '/content/'\n","          pattern = \"chunk-??.wav\"\n","          min_cost_chunk = \"\"\n","          current_cost = np.inf\n","          for chunk in glob.glob(pattern):\n","            path_chunk = directory + chunk\n","            path_gt = \"/content/mmm_GTT.wav\"\n","            temp = compare_mfcc_features(path_mfcc1=path_chunk, path_mfcc2=path_gt)\n","            print(f'Comparing GT to {chunk}. score: {temp:.1f}')\n","            if(temp < current_cost):\n","              current_cost = temp\n","              min_cost_chunk = chunk\n","          df.loc[[padding_duration_it],frame_duration_it] += get_num_of_chunk_as_int(min_cost_chunk) - 9\n","          # print(df)\n","\n","      if os.path.isfile('/content/chunk-00.wav'):\n","        print(\"last Delete\")\n","        !rm chunk-??.wav\n","  return df.style.applymap(color_cells)"],"metadata":{"id":"KcptnUJK1aC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def file_params_VAD_iterate_cross_patient_comparison(aggressive_list=[3],audio_path=None,frame_duration_list=range(10,40,10),padding_duration_list=range(100,550,50)):\n","  print(f'Filename: {audio_path}')\n","  for aggressive_it in aggressive_list: #iterate over list\n","      for frame_duration_it in frame_duration_list: #iterate over list\n","        column_list = []\n","        for padding_duration_it in padding_duration_list: #iterate over list\n","          # Run VAD on said parameters\n","          print(f'Params: aggr: {aggressive_it} frame_ms: {frame_duration_it} padding: {padding_duration_it}')\n","          chunks_web = webrtcvad_usage(aggressive=aggressive_it,audio_path=audio_path,frame_duration=frame_duration_it,padding_duration=padding_duration_it,min_chunk_length = 0.85)\n","          # calculate DTW score between GT and current MFCC features\n","\n","          directory = '/content/'\n","          pattern = \"chunk-??.wav\"\n","          min_cost_chunk = \"\"\n","          current_cost = np.inf\n","          chunk_list = [\"chunk-08.wav\",\"chunk-09.wav\",\"chunk-10.wav\",\"chunk-11.wav\"]\n","\n","          for chunk in chunk_list:\n","            path_chunk = directory + chunk\n","            if(os.path.isfile(path_chunk) == False ):\n","              continue\n","            temp = 0\n","            debug_df = pd.DataFrame(0,columns = ['127','01','97','151'],index =[\"scores\"])\n","\n","            for gt_chunk in glob.glob('/content/GT/*.wav'):\n","              print(f\"\\nComparing new patient chunk {chunk.split('-')[1].split('.')[0]} to \", gt_chunk.split('/')[3])\n","              gt_id = gt_chunk.split('/')[3].split(\"_\")[1]\n","              score = compare_mfcc_features(path_mfcc1=path_chunk, path_mfcc2=gt_chunk)\n","              # score = libroa_dtw(path_mfcc1=path_chunk, path_mfcc2=gt_chunk)\n","              debug_df.loc[[\"scores\"],gt_id] = score\n","              temp += score\n","            print(debug_df)\n","            temp = temp/4\n","            print(\"average score is \", temp)\n","\n","            if(temp < current_cost):\n","              current_cost = temp\n","              min_cost_chunk = chunk\n","              print(\"\\nNew min is \", get_num_of_chunk_as_int(min_cost_chunk),\" with a score of \", current_cost)\n","          df.loc[[padding_duration_it],frame_duration_it] += int(get_num_of_chunk_as_int(min_cost_chunk))\n","          # print(df)\n","\n","      if os.path.isfile('/content/chunk-00.wav'):\n","        print(\"last Delete\")\n","        !rm chunk-??.wav\n","  return df.style.applymap(color_cells)"],"metadata":{"id":"z0yxljICl2oO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#                       F - Scar                             M - Scar                  M - Healthy                        F - Healthy\n","audio_paths_list = ['/content/pat_127_sox_mono.wav','/content/pat_01_sox_mono.wav','/content/pat_97_sox_mono.wav','/content/pat_151_sox_mono.wav']"],"metadata":{"id":"yne9h4c1ufh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the ground truth for chunk #9 - the \"mmm\" audio from patient 127\n","if (not os.path.isdir(\"/content/GT\")):\n","  os.mkdir(\"/content/GT\")\n","\n","\n","chunks_web = webrtcvad_usage(aggressive=3,audio_path=audio_paths_list[0],frame_duration=10,padding_duration=100,min_chunk_length=0.8)\n","os.rename('/content/chunk-09.wav', \"/content/GT/pat_127_GT.wav\")\n","\n","\n","chunks_web = webrtcvad_usage(aggressive=3,audio_path=audio_paths_list[1],frame_duration=10,padding_duration=100,min_chunk_length=0.8)\n","os.rename('/content/chunk-09.wav', \"/content/GT/pat_01_GT.wav\")\n","\n","\n","chunks_web = webrtcvad_usage(aggressive=3,audio_path=audio_paths_list[2],frame_duration=10,padding_duration=100,min_chunk_length=0.8)\n","os.rename('/content/chunk-09.wav', \"/content/GT/pat_97_GT.wav\")\n","\n","\n","chunks_web = webrtcvad_usage(aggressive=3,audio_path=audio_paths_list[3],frame_duration=10,padding_duration=100,min_chunk_length=0.8)\n","os.rename('/content/chunk-09.wav', \"/content/GT/pat_151_GT.wav\")\n","\n","!rm chunk-??.wav"],"metadata":{"id":"HYcw-sZoKUR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pat_num ='74'\n","\n","!sox /content/drive/MyDrive/Study_materials/Voice\\ samples\\ study/patient\\ {pat_num}.wav -c 1 pat_{pat_num}_sox.wav\n","!sox /content/drive/MyDrive/Study_materials/Voice\\ samples\\ study/Patient\\ {pat_num}.wav -c 1 pat_{pat_num}_sox.wav\n","!sox /content/pat_{pat_num}_sox.wav -r 16000 /content/pat_{pat_num}_sox_mono.wav\n","!rm pat_{pat_num}_sox.wav\n"],"metadata":{"id":"shEHe2dGq4ZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = {}\n","df = pd.DataFrame(0,columns = [10,20],index =range(100,350,50))\n","path = '/content/pat_'+pat_num+'_sox_mono.wav'\n","num = path.split(\"_\")[1]\n","a[num] = file_params_VAD_iterate_cross_patient_comparison(audio_path=path,frame_duration_list=[10,20],padding_duration_list=range(100,350,50))\n"],"metadata":{"id":"Vt2XlKS4qxib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a['74']"],"metadata":{"id":"J08M7cO4cZuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for path in audio_paths_list:\n","  # a = file_params_VAD_iterate(audio_path=path,frame_duration_list=range(20,40,10),padding_duration_list=range(400,600,100))\n","  df = pd.DataFrame(0,columns = [10,20,30],index =range(100,550,50))\n","  num = path.split(\"_\")[1]\n","  a[num] = file_params_VAD_iterate(audio_path=path)"],"metadata":{"id":"jM2Edd_W5XE7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chunks_web = webrtcvad_usage(aggressive=3,audio_path='/content/pat_'+pat_num+'_sox_mono.wav',frame_duration=20,padding_duration=150,min_chunk_length = 0.85)"],"metadata":{"id":"PrZyndKwZfp9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_present = pd.DataFrame(0,columns = [\"01\",\"127\",\"151\",\"97\"],index =range(0,18,1))\n","\n","directory = '/content/'\n","pattern = \"chunk-??.wav\"\n","# chunk_list = [\"chunk-08.wav\",\"chunk-09.wav\",\"chunk-10.wav\",\"chunk-11.wav\"]\n","\n","for chunk in glob.glob('/content/chunk-??.wav'):\n","  print(chunk)\n","  path_chunk = chunk\n","  if(os.path.isfile(path_chunk) == False ):\n","    continue\n","\n","  for gt_chunk in glob.glob('/content/GT/*.wav'):\n","    print(f\"\\nComparing new patient chunk {get_num_of_chunk_as_int(chunk)} to \", gt_chunk.split('/')[3])\n","    gt_id = gt_chunk.split('/')[3].split(\"_\")[1]\n","    score = compare_mfcc_features(path_mfcc1=path_chunk, path_mfcc2=gt_chunk)\n","    df_present.loc[[int(get_num_of_chunk_as_int(chunk))],gt_id] = score\n","  print(df_present)"],"metadata":{"id":"BVyob-Jm3GBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.options.display.float_format = '{:,.3f}'.format\n","df_present\n","temps = pd.DataFrame(1,columns = [\"avg\"],index =range(0,18,1))\n","f = df_present.div(df_present.sum(axis=0), axis=1)\n","temps = f.mean(axis=1)\n","print(temps)\n","# res = pd.concat([f,temps],axis=1)\n","# res"],"metadata":{"id":"Ir76RCpE5Sfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a[\"74\"]"],"metadata":{"id":"rJNpL3E-85nq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dftest = pd.DataFrame(0,columns = [\"01\",\"127\",\"151\",\"97\"],index =range(0,18,1))\n","dftest2 = pd.DataFrame(0,columns = [\"01\",\"127\",\"151\",\"97\"],index =range(0,18,1))\n","dftest"],"metadata":{"id":"oDXU0vKlON87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory = '/content/'\n","pattern = \"chunk-??.wav\"\n","\n","for new_pat_chunk in glob.glob('/content/chunk-??.wav'):\n","  new_pat_num = int(get_num_of_chunk_as_int(new_pat_chunk))\n","  print(f'Comparing {new_pat_num} ',end =\"\")\n","  for gt_chunk in glob.glob('/content/GT/*.wav'):\n","    gt_num = gt_chunk.split(\"_\")[1]\n","    print(f'{gt_num}, ',end =\"\")\n","    score = libroa_dtw(path_mfcc1=new_pat_chunk, path_mfcc2=gt_chunk)\n","    mfccscore = compare_mfcc_features(path_mfcc1=new_pat_chunk, path_mfcc2=gt_chunk)\n","    dftest.loc[[new_pat_num],gt_num] = score\n","    dftest2.loc[[new_pat_num],gt_num] = mfccscore\n","  print(\"\\n\")"],"metadata":{"id":"Rfq9Wt80OdXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.options.display.float_format = '{:,.2f}'.format\n","dftest2"],"metadata":{"id":"ZWZYYC1OUE1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["waveform, sample_rate = torchaudio.load('chunk-09.wav')\n","Audio(waveform.numpy()[0], rate=16000)\n","\n","# audio_labels = [\"0 normal pitch E\",\"1 low pitch E\",\"2 high pitch E\",\n","                # \"3 normal pitch Ah\",\"4 low pitch Ah\",\"5 high pitch Ah\",\n","                # \"6 normal pitch Ooh\",\"7 low pitch Ooh\",\"8 high pitch Ooh\",\n","                # \"9 normal pitch Mmmmm\"]"],"metadata":{"id":"9HoanHYqIInq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["waveform2, sample_rate = torchaudio.load('/content/GT/pat_97_GT.wav')\n","Audio(waveform2.numpy()[0], rate=16000)\n"],"metadata":{"id":"buyPuCJhdp94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["waveform4, sample_rate = torchaudio.load('chunk-11.wav')\n","Audio(waveform4.numpy()[0], rate=16000)"],"metadata":{"id":"RJtEiW4rQu3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_specgram(waveform, sample_rate, title=\"Spectrogram\"):\n","    waveform = waveform.numpy()\n","\n","    num_channels, num_frames = waveform.shape\n","\n","    figure, axes = plt.subplots(num_channels, 1)\n","    if num_channels == 1:\n","        axes = [axes]\n","    for c in range(num_channels):\n","        axes[c].specgram(waveform[c], Fs=sample_rate)\n","        if num_channels > 1:\n","            axes[c].set_ylabel(f\"Channel {c+1}\")\n","    figure.suptitle(title)\n","    plt.show(block=False)"],"metadata":{"id":"LCVvJM2nKK70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Audio(waveform2.numpy()[0],rate=16000)"],"metadata":{"id":"rZM-vHHXMEVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import speech_recognition as sr\n","\n","filename = \"/content/chunk-09.wav\"\n","\n","# initialize the recognizer\n","r = sr.Recognizer()\n","\n","# open the file\n","with sr.AudioFile(filename) as source:\n","    # listen for the data (load audio to memory)\n","    audio_data = r.record(source)\n","    # recognize (convert from speech to text)\n","    text = r.recognize_google(audio_data)\n","    print(text)\n"],"metadata":{"id":"GO_Se0D_N3gr"},"execution_count":null,"outputs":[]}]}