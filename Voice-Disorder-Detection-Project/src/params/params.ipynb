{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17058,"status":"ok","timestamp":1716818258422,"user":{"displayName":"Or Beyar","userId":"10215772442836299100"},"user_tz":-180},"id":"ZwLBjg69FjeM","outputId":"4b998378-206b-43c4-e682-549a149b86de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","import sys\n","drive.mount('/content/drive/',force_remount = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AeapMOu7FhqU"},"outputs":[],"source":["from random import sample\n","import torch\n","from torch.nn import functional as F\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","import torchaudio.transforms as ta_trans"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"N4oeWyTqFrCA","executionInfo":{"status":"ok","timestamp":1716908246974,"user_tz":-180,"elapsed":1,"user":{"displayName":"Or Beyar","userId":"10215772442836299100"}}},"outputs":[],"source":["\n","'''\n","hyperparams for VGGish and YAMNet, plus common configs\n","vggish params are retrieved from:\n","    https://github.com/tensorflow/models/blob/bd488858d610e44df69da6f89277e9de8a03722c/research/audioset/vggish/vggish_params.py\n","yamnet params are retrieved from:\n","    https://github.com/tensorflow/models/blob/bd488858d610e44df69da6f89277e9de8a03722c/research/audioset/yamnet/params.py\n","'''\n","import json\n","\n","# project_dir = {\n","#     \"sipl-gpu2-u.staff.technion.ac.il\":\"/home/chenka/VoiceDisorerIdentification\"\n","# }\n","\n","data_location = {'raw_data':'/content/drive/MyDrive/Study_materials/Voice_disorder_detection_project/data/raw_data/patients',\n","                 'data_spreadsheet':'/content/drive/MyDrive/Study_materials/Voice_disorder_detection_project/data/raw_data/patients/CRF table.xlsx',\n","                     'preprocessed_data':'/content/drive/MyDrive/Study_materials/Voice_disorder_detection_project/data/preproccessed_data'\n","                     }\n","\n","class Pathologies():\n","  def __init__(self):\n","    pat_data_sheet = pd.read_excel(data_location['data_spreadsheet'])\n","    pat_data_sheet = pat_data_sheet[pat_data_sheet['Age'].notna()]\n","    # Create a bin list of known diseases\n","    list_of_diseases = []\n","    for index, row in pat_data_sheet.iterrows():\n","      diagnosis = row['dysphonia diagnosis']\n","      if type(diagnosis) == str:\n","        for word in diagnosis.split(\"+\"):\n","          stripped = word.strip()\n","          if stripped not in list_of_diseases:\n","            list_of_diseases.append(stripped)\n","\n","    list_of_diseases = ['Healthy' if disease.lower() == 'none' else disease for disease in list_of_diseases]\n","    disease_to_int = {}\n","    for i, disease in enumerate(list_of_diseases):\n","      if disease == 'Healthy':\n","        disease_to_int[disease] = 0\n","      else:\n","        disease_to_int[disease] = i + 1\n","    PathologiesToIndex = {k.lower(): v for k, v in sorted(disease_to_int.items(), key=lambda item: item[1])}\n","    self.PathologiesToIndex = PathologiesToIndex\n","\n","  def get_pathologies_to_index(self):\n","    return self.PathologiesToIndex\n","\n","  def print(self):\n","    for key,val in self.PathologiesToIndex.items():\n","      print(f'{key}: {val}')\n","class webrtcvadParams():\n","  AGGRESSIVE = 3\n","  FRAME_DURATION = 10   # in ms\n","  PADDING_DURATION = 150\n","  MIN_CHUNK_LENGTH = 0.90\n","\n","class CommonParams():\n","    # for STFT\n","    TARGET_SAMPLE_RATE = 16000\n","    STFT_WINDOW_LENGTH_SECONDS = 0.025\n","    STFT_HOP_LENGTH_SECONDS = 0.010\n","\n","    # for log mel spectrogram\n","    NUM_MEL_BANDS = 64\n","    MEL_MIN_HZ = 125\n","    MEL_MAX_HZ = 7500\n","    LOG_OFFSET = 0.001  # NOTE 0.01 for vggish, and 0.001 for yamnet\n","\n","    # convert input audio to segments\n","    PATCH_WINDOW_IN_SECONDS = 0.48\n","\n","    # largest feedforward chunk size at test time\n","    VGGISH_CHUNK_SIZE = 128\n","    YAMNET_CHUNK_SIZE = 256\n","\n","    # num of data loading threads\n","    NUM_LOADERS = 4\n","\n","    VOICE_SAMPLE_MIN_LENGTH = 0.96\n","    SVD_SAMPLE_RATE = 50000\n","\n","class YAMNetParams():\n","    # Copyright 2019 The TensorFlow Authors All Rights Reserved.\n","    #\n","    # Licensed under the Apache License, Version 2.0 (the \"License\");\n","    # you may not use this file except in compliance with the License.\n","    # You may obtain a copy of the License at\n","    #\n","    #     http://www.apache.org/licenses/LICENSE-2.0\n","    #\n","    # Unless required by applicable law or agreed to in writing, software\n","    # distributed under the License is distributed on an \"AS IS\" BASIS,\n","    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","    # See the License for the specific language governing permissions and\n","    # limitations under the License.\n","    # ==============================================================================\n","\n","    \"\"\"Hyperparameters for YAMNet.\"\"\"\n","\n","    # The following hyperparameters (except PATCH_HOP_SECONDS) were used to train YAMNet,\n","    # so expect some variability in performance if you change these. The patch hop can\n","    # be changed arbitrarily: a smaller hop should give you more patches from the same\n","    # clip and possibly better performance at a larger computational cost.\n","    SAMPLE_RATE = 16000\n","    STFT_WINDOW_SECONDS = 0.025\n","    STFT_HOP_SECONDS = 0.010\n","    MEL_BANDS = 64\n","    MEL_MIN_HZ = 125\n","    MEL_MAX_HZ = 7500\n","    LOG_OFFSET = 0.001\n","    PATCH_WINDOW_SECONDS = 0.48\n","    PATCH_HOP_SECONDS = 0.2\n","\n","    PATCH_FRAMES = int(round(PATCH_WINDOW_SECONDS / STFT_HOP_SECONDS))\n","    PATCH_BANDS = MEL_BANDS\n","    NUM_CLASSES = 521\n","    CONV_PADDING = 'same'\n","    BATCHNORM_CENTER = True\n","    BATCHNORM_SCALE = False\n","    BATCHNORM_EPSILON = 1e-4\n","    CLASSIFIER_ACTIVATION = 'sigmoid'\n","\n","    FEATURES_LAYER_NAME = 'features'\n","    EXAMPLE_PREDICTIONS_LAYER_NAME = 'predictions'\n","\n","\n","# NOTE for our inference, don't need overlapping windows\n","# YAMNetParams.PATCH_HOP_SECONDS = YAMNetParams.PATCH_WINDOW_SECONDS\n","YAMNetParams.PATCH_HOP_SECONDS = 1.0\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM1YTkfY8/8JM3ul7jAbKAt"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}