{"cells":[{"cell_type":"markdown","metadata":{"id":"Kjlvl8_Zwc9z"},"source":["Load data from drive"]},{"cell_type":"code","source":["from google.colab import drive\n","import import_ipynb\n","import sys\n","\n","drive.mount('/content/drive/',force_remount = False)\n","proj_dir_path = '/content/drive/MyDrive/Study_materials/Voice disorder detection project/'\n","sys.path.append(proj_dir_path)\n","%cd $proj_dir_path\n","\n","\n","my_voice_data_no_spaces = '/content/drive/MyDrive/Study_materials/Voice disorder detection project/patients'\n","my_voice_data = '/content/drive/MyDrive/Study_materials/Voice\\ disorder\\ detection\\ project/patients'"],"metadata":{"id":"qce85OxqN8V1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y4hKYmIwwgdJ"},"source":["imports and installations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dH2NHarDwspu"},"outputs":[],"source":["# install google's VAD\n","!pip install webrtcvad\n","\n","#install librosa and sox\n","!apt-get install libsox-fmt-all\n","!apt-get install sox\n","!pip install sox\n","!pip install dill"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10250,"status":"ok","timestamp":1715006305429,"user":{"displayName":"Or Beyar","userId":"10215772442836299100"},"user_tz":-180},"id":"KeRiGDq2F7Y3","outputId":"1833cd22-a94e-40d1-c58b-86f43e2aabcc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (0.3.8)\n"]}],"source":["!pip install -- dill"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5566,"status":"ok","timestamp":1715006310992,"user":{"displayName":"Or Beyar","userId":"10215772442836299100"},"user_tz":-180},"id":"ptTBjLmnwcDu","outputId":"407e983c-d77d-48ea-b1c3-23ff8da210b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.1+cu121\n","2.2.1+cu121\n","cpu\n"]}],"source":["\n","## Imports\n","\n","import os\n","import glob\n","import numpy as np\n","import torch\n","import torchaudio\n","import torchaudio.functional as F\n","import torchaudio.transforms as T\n","import matplotlib.pyplot as plt\n","import librosa\n","import soundfile as sf\n","from IPython.display import Audio\n","import pandas as pd\n","from scipy.io import wavfile\n","import re\n","import dill\n","# from sox.file_info import sample_rate\n","from datetime import date\n","import shutil\n","\n","\n","import webrtcvad\n","import sox\n","\n","print(torch.__version__)\n","print(torchaudio.__version__)\n","\n","## Use cuda if available\n","torch.random.manual_seed(0)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"9Ge6DeVVxCUh"},"source":["Constants and helper func definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdVT77i7wqTN"},"outputs":[],"source":["# Define constants\n","wanted_sample_rate = 16000\n","\n","audio_labels_compact = [\"E\",\"LE\",\"HE\",\n","                \"Ah\",\"LAh\",\"HAh\",\n","                \"Ooh\",\"LOoh\",\"HOoh\",\n","                \"Mmm\", \"Sen\",\"Diag\"]\n","audio_labels = [\"normal pitch E\",\"low pitch E\",\"high pitch E\",\n","                \"normal pitch Ah\",\"low pitch Ah\",\"high pitch Ah\",\n","                \"normal pitch Ooh\",\"low pitch Ooh\",\"high pitch Ooh\",\n","                \"normal pitch Mmmmm\",\"Sentence\",\"Diagnosis\"]\n","\n","def get_num_of_chunk_as_int(chunk):\n","  # receives a chunk's name in str and extracts the num as integer\n","  # expects the chunk name to be in the a specific format\n","  return (chunk.split(\".\")[0]).split(\"-\")[1]\n","\n","class Chunk_obj:\n","  # patient_num - Patient's number\n","  # chunk_num - patient's audio chunk number (chronological)\n","  # sample_rate - chunk's sample rate\n","  # data - The chunk's recording data - np array\n","  def __init__(self, patient_num,chunk_num, sample_rate,data):\n","    self.patient_num = patient_num\n","    if (chunk_num <= 9):\n","      self.label = audio_labels_compact[chunk_num]\n","    else:\n","      self.label = audio_labels[10]\n","    self.sample_rate = sample_rate\n","    self.data = data\n","\n","  def get_data(self):\n","    return self.data\n","\n","  def __str__(self):\n","    return f\" P-{self.patient_num} {self.label} \""]},{"cell_type":"markdown","metadata":{"id":"dP_QA5BNxMan"},"source":["Google's VAD wrapper methods"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFdeZ4Aew_Ai"},"outputs":[],"source":["import collections\n","import contextlib\n","import sys\n","import wave\n","\n","def read_wave(path):\n","    \"\"\"Reads a .wav file.\n","\n","    Takes the path, and returns (PCM audio data, sample rate).\n","    \"\"\"\n","    with contextlib.closing(wave.open(path, 'rb')) as wf:\n","        num_channels = wf.getnchannels()\n","        assert num_channels == 1\n","        sample_width = wf.getsampwidth()\n","        assert sample_width == 2\n","        sample_rate = wf.getframerate()\n","        assert sample_rate in (8000, 16000, 32000, 48000)\n","        pcm_data = wf.readframes(wf.getnframes())\n","        return pcm_data, sample_rate\n","\n","\n","def write_wave(path, audio, sample_rate):\n","    \"\"\"Writes a .wav file.\n","\n","    Takes path, PCM audio data, and sample rate.\n","    \"\"\"\n","    with contextlib.closing(wave.open(path, 'wb')) as wf:\n","        wf.setnchannels(1)\n","        wf.setsampwidth(2)\n","        wf.setframerate(sample_rate)\n","        wf.writeframes(audio)\n","\n","\n","\n","class Frame(object):\n","    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n","    def __init__(self, bytes, timestamp, duration):\n","        self.bytes = bytes\n","        self.timestamp = timestamp\n","        self.duration = duration\n","\n","\n","def frame_generator(frame_duration_ms, audio, sample_rate):\n","    \"\"\"Generates audio frames from PCM audio data.\n","\n","    Takes the desired frame duration in milliseconds, the PCM data, and\n","    the sample rate.\n","\n","    Yields Frames of the requested duration.\n","    \"\"\"\n","    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n","    offset = 0\n","    timestamp = 0.0\n","    duration = (float(n) / sample_rate) / 2.0\n","    while offset + n < len(audio):\n","        yield Frame(audio[offset:offset + n], timestamp, duration)\n","        timestamp += duration\n","        offset += n\n","\n","\n","def vad_collector(sample_rate, frame_duration_ms,\n","                  padding_duration_ms, vad, frames):\n","    \"\"\"Filters out non-voiced audio frames.\n","\n","    Given a webrtcvad.Vad and a source of audio frames, yields only\n","    the voiced audio.\n","\n","    Uses a padded, sliding window algorithm over the audio frames.\n","    When more than 90% of the frames in the window are voiced (as\n","    reported by the VAD), the collector triggers and begins yielding\n","    audio frames. Then the collector waits until 90% of the frames in\n","    the window are unvoiced to detrigger.\n","\n","    The window is padded at the front and back to provide a small\n","    amount of silence or the beginnings/endings of speech around the\n","    voiced frames.\n","\n","    Arguments:\n","\n","    sample_rate - The audio sample rate, in Hz.\n","    frame_duration_ms - The frame duration in milliseconds.\n","    padding_duration_ms - The amount to pad the window, in milliseconds.\n","    vad - An instance of webrtcvad.Vad.\n","    frames - a source of audio frames (sequence or generator).\n","\n","    Returns: A generator that yields PCM audio data.\n","    \"\"\"\n","    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n","    # We use a deque for our sliding window/ring buffer.\n","    ring_buffer = collections.deque(maxlen=num_padding_frames)\n","    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n","    # NOTTRIGGERED state.\n","    triggered = False\n","\n","    voiced_frames = []\n","    for frame in frames:\n","        is_speech = vad.is_speech(frame.bytes, sample_rate)\n","\n","        sys.stdout.write('1' if is_speech else '0')\n","        if not triggered:\n","            ring_buffer.append((frame, is_speech))\n","            num_voiced = len([f for f, speech in ring_buffer if speech])\n","            # If we're NOTTRIGGERED and more than 90% of the frames in\n","            # the ring buffer are voiced frames, then enter the\n","            # TRIGGERED state.\n","            if num_voiced > 0.9 * ring_buffer.maxlen:\n","                triggered = True\n","                sys.stdout.write('+(%s)' % (ring_buffer[0][0].timestamp,))\n","                # We want to yield all the audio we see from now until\n","                # we are NOTTRIGGERED, but we have to start with the\n","                # audio that's already in the ring buffer.\n","                for f, s in ring_buffer:\n","                    voiced_frames.append(f)\n","                ring_buffer.clear()\n","        else:\n","            # We're in the TRIGGERED state, so collect the audio data\n","            # and add it to the ring buffer.\n","            voiced_frames.append(frame)\n","            ring_buffer.append((frame, is_speech))\n","            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n","            # If more than 90% of the frames in the ring buffer are\n","            # unvoiced, then enter NOTTRIGGERED and yield whatever\n","            # audio we've collected.\n","            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n","                sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n","                triggered = False\n","                yield b''.join([f.bytes for f in voiced_frames])\n","                ring_buffer.clear()\n","                voiced_frames = []\n","    if triggered:\n","        sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n","    sys.stdout.write('\\n')\n","    # If we have any leftover voiced audio when we run out of input,\n","    # yield it.\n","    if voiced_frames:\n","        yield b''.join([f.bytes for f in voiced_frames])\n","\n","def check_chunk_files():\n","  chunk_list = glob.glob('/content/chunk-??.wav')\n","  if len(chunk_list) > 0:\n","    return True\n","  else:\n","    return False"]},{"cell_type":"markdown","metadata":{"id":"ABF7hD4VxZzn"},"source":["Google VAD usage method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSy2PJL6xULw"},"outputs":[],"source":["def webrtcvad_usage(aggressive=0, audio_path=None,frame_duration=30,padding_duration=300,min_chunk_length = 0.5):\n","    if os.path.isfile('/content/chunk-00.wav'):\n","      print(\"Clearing previous .wav files\")\n","      !rm chunk-??.wav\n","      !rm chunk-???.wav\n","    vad = webrtcvad.Vad(aggressive)\n","    audio, sample_rate  = read_wave(audio_path)\n","    frames = frame_generator(frame_duration, audio, sample_rate)\n","    frames = list(frames)\n","    segments = vad_collector(sample_rate, frame_duration, padding_duration, vad, frames)\n","\n","\n","\n","  #  sample_rate - The audio sample rate, in Hz.\n","  #   frame_duration_ms - The frame duration in milliseconds.\n","  #   padding_duration_ms - The amount to pad the window, in milliseconds.\n","  #   vad - An instance of webrtcvad.Vad.\n","  #   frames - a source of audio frames (sequence or generator).\n","    i = 0\n","    for j,segment in enumerate(segments):\n","      path = 'chunk-%002d.wav' % (i,)\n","      # print(' Writing %s' % (path,))\n","      len_sec = len(segment)/(sample_rate*2)\n","      # if( i < 10):\n","      if((len_sec >= min_chunk_length) ):\n","        write_wave(path, segment, sample_rate)\n","        # print(f'\\n phone Chunk {i} length is {len_sec} seconds')\n","        i = i+1\n","      # else:\n","        # print(f'\\n too short! Chunk {i} length is {len_sec} seconds')\n","    return segments"]},{"cell_type":"markdown","metadata":{"id":"L2dFr_PqyTHj"},"source":["Given a patient number, run it through sox, resampling to 16k and converting to mono-channel"]},{"cell_type":"markdown","source":["# Phase 1\n","each vowel gets it's own label\n","\n","# Phase 2\n","Cleaning noise from data, and splitting the cleaned .wav file into"],"metadata":{"id":"kAMsMKFYd-bG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjRUOzhhySt9"},"outputs":[],"source":["#Phase 1\n","def audio_split_phase1(pat_num = -1):\n","  if(pat_num == -1):\n","    print(\"No pat num or df given\")\n","    return\n","\n","    ## Sox usage - resample and monochannel\n","  # file name can start with capital P or lowercase p, we take the monochannel\n","  !sox {my_voice_data}/Patient\\ {pat_num}.wav -c 1 pat_{pat_num}_sox.wav\n","  !sox {my_voice_data}/patient\\ {pat_num}.wav -c 1 pat_{pat_num}_sox.wav\n","  # resample to 16k\n","  !sox /content/pat_{pat_num}_sox.wav -r 16000 /content/pat_{pat_num}_sox_mono.wav\n","  # delete older file\n","  !rm pat_{pat_num}_sox.wav\n","\n","  # google VAD usage\n","  path = 'pat_'+ str(pat_num)+'_sox_mono.wav'\n","  webrtcvad_usage(aggressive=3, audio_path = path,frame_duration=10,padding_duration=150,min_chunk_length = 0.90)\n","  chunk_list = [None] * 12\n","  cbn=sox.Combiner()\n","  sounds=[]\n","  for chunk in glob.glob('/content/chunk-??.wav'): ## we assume there are less than 100 chunks\n","    chunk_num = int(get_num_of_chunk_as_int(chunk))\n","    if(chunk_num > 9): # chunk 10+ are being merged into a single sentence\n","      sounds.append(chunk)\n","    else:                                        # chunk 0-9 are being labled into df\n","      sample_rate, temp_wav = wavfile.read(chunk)\n","      print(\"sample_rate is \",sample_rate)\n","      chunk_obj = Chunk_obj(patient_num=pat_num,chunk_num =chunk_num,sample_rate=sample_rate,data=temp_wav)\n","      chunk_list[chunk_num] = chunk_obj\n","\n","  #PROCESS SOUND PATHS TO AN ARRAY\n","  if len(sounds)>=2:\n","      print(sounds)\n","      cbn.build(sounds,'sentence.wav','concatenate')\n","\n","  # insert sentence.wav into df\n","  # Create chunkObj\n","  try:\n","    _, temp_wav_sentence = wavfile.read('/content/sentence.wav')\n","    sentence_chunk_obj = Chunk_obj(patient_num=pat_num,chunk_num = 11,sample_rate=sample_rate,data=temp_wav_sentence)\n","    chunk_list[10] = sentence_chunk_obj\n","  except:\n","    chunk_list[10] = \"No sentence audio\"\n","\n","\n","\n","  !rm pat_{pat_num}_sox_mono.wav\n","  !rm sentence.wav\n","  return chunk_list\n","\n","def audio_split_phase2(pat_num = -1,aggressive=3,frame_duration=10,padding_duration=150,save_to_drive= False):\n","  if(pat_num == -1):\n","    print(\"No pat num or df given\")\n","    return\n","\n","    ## Sox usage - resample and monochannel\n","  # file name can start with capital P or lowercase p, we take the monochannel\n","  !sox {my_voice_data}/Patient\\ {pat_num}.wav -c 1 pat_{pat_num}_sox.wav\n","  !sox {my_voice_data}/patient\\ {pat_num}.wav -c 1 pat_{pat_num}_sox.wav\n","  # resample to 16k\n","  !sox /content/pat_{pat_num}_sox.wav -r 16000 /content/pat_{pat_num}_sox_mono.wav\n","  # delete older file\n","  !rm pat_{pat_num}_sox.wav\n","\n","  # google VAD usage\n","  path = 'pat_'+str(pat_num)+'_sox_mono.wav'\n","  webrtcvad_usage(aggressive=3, audio_path=path,frame_duration=10,padding_duration=150,min_chunk_length = 0.90)\n","  chunk_list = [None] * 100\n","  cbn=sox.Combiner()\n","  if(check_chunk_files()):\n","    for chunk in glob.glob('/content/chunk-??.wav'): ## we assume there are less than 100 chunks\n","      chunk_num = int(get_num_of_chunk_as_int(chunk))\n","      chunk_list[chunk_num] = chunk\n","  else:\n","    print(f\"Recording {pat_num} is too noisy to be split\")\n","    return [None]\n","\n","\n","  # Combine all chunks into a single file\n","  cbn=sox.Combiner()\n","  chunk_list = [x for x in chunk_list if x is not None]\n","  if len(chunk_list)>=1:\n","      print(chunk_list)\n","      cbn.build(chunk_list,'test.wav','concatenate')\n","\n","  try:\n","    if(save_to_drive):\n","      source_path = '/content/test.wav'\n","      name = \"patient_\" + str(pat_num)+\".wav\"\n","      destination_path = '/content/drive/MyDrive/Study_materials/Voice disorder detection project/patients_preproccessed/'+name\n","      # Move and rename the file\n","      shutil.move(source_path, destination_path)\n","    else:\n","      sample_rate, temp_wav_sentence = wavfile.read('/content/test.wav')\n","      sentence_chunk_obj = Chunk_obj(patient_num=pat_num,chunk_num = 10,sample_rate=sample_rate,data=temp_wav_sentence)\n","  except Exception as e:\n","    print(\"Something happened:\", e)\n","\n","\n","\n","  !rm pat_{pat_num}_sox_mono.wav\n","  !rm test.wav\n","  !rm chunk-**.wav\n","  !rm chunk-***.wav\n","  return\n","\n","\n","def audio_split_phase3(pat_num = -1,aggressive=3,frame_duration=10,padding_duration=150,save_to_drive= False):\n","  if(pat_num == -1):\n","    print(\"No pat num or df given\")\n","    return\n","\n","    ## Sox usage - resample and monochannel\n","  # file name can start with capital P or lowercase p, we take the monochannel\n","  !sox {my_voice_data}/Patient\\ {pat_num}.wav -c 1 pat_{pat_num}_sox.wav\n","  !sox {my_voice_data}/patient\\ {pat_num}.wav -c 1 pat_{pat_num}_sox.wav\n","  # resample to 16k\n","  !sox /content/pat_{pat_num}_sox.wav -r 16000 /content/pat_{pat_num}_sox_mono.wav\n","  # delete older file\n","  !rm pat_{pat_num}_sox.wav\n","\n","  # google VAD usage\n","  path = 'pat_'+str(pat_num)+'_sox_mono.wav'\n","  webrtcvad_usage(aggressive=3, audio_path=path,frame_duration=10,padding_duration=150,min_chunk_length = 0.90)\n","  chunk_list = [None] * 100\n","  cbn=sox.Combiner()\n","  if(check_chunk_files()):\n","    for chunk in glob.glob('/content/chunk-??.wav'): ## we assume there are less than 100 chunks\n","      chunk_num = int(get_num_of_chunk_as_int(chunk))\n","      chunk_list[chunk_num] = chunk\n","  else:\n","    print(f\"Recording {pat_num} is too noisy to be split\")\n","    return [None]\n","\n","\n","  # Combine all chunks into a single file\n","  cbn=sox.Combiner()\n","  chunk_list = [x for x in chunk_list if x is not None]\n","  if len(chunk_list)>=1:\n","      print(chunk_list)\n","      cbn.build(chunk_list,'test.wav','concatenate')\n","\n","  try:\n","    if(save_to_drive):\n","      source_path = '/content/test.wav'\n","      name = \"patient_\" + str(pat_num)+\".wav\"\n","      destination_path = '/content/drive/MyDrive/Study_materials/Voice disorder detection project/patients_snippets/'+name\n","      # Move and rename the file\n","      shutil.move(source_path, destination_path)\n","    else:\n","      sample_rate, temp_wav_sentence = wavfile.read('/content/test.wav')\n","      sentence_chunk_obj = Chunk_obj(patient_num=pat_num,chunk_num = 10,sample_rate=sample_rate,data=temp_wav_sentence)\n","  except Exception as e:\n","    print(\"Something happened:\", e)\n","\n","\n","\n","  !rm pat_{pat_num}_sox_mono.wav\n","  !rm test.wav\n","  !rm chunk-**.wav\n","  !rm chunk-***.wav\n","  return"]},{"cell_type":"code","source":["# prompt: split an audio file into X equal length files using sox\n","\n","!sox input.wav output%03d.wav trim 0 5\n"],"metadata":{"id":"WAtijDPnXQOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["audio_split_phase2(pat_num=15,save_to_drive=True)"],"metadata":{"id":"PMzsxcA36WLQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6628,"status":"ok","timestamp":1713900496199,"user":{"displayName":"Or Beyar","userId":"10215772442836299100"},"user_tz":-180},"id":"I-Y9oGYenk3D","outputId":"6be558f9-88f0-4ff6-f02b-ead77a4bdc4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"]}],"source":["!pip install xlrd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBemdMZKoVYA"},"outputs":[],"source":["file_path = '/content/drive/MyDrive/Study_materials/Voice disorder detection project/patients/CRF table.xlsx'\n","patients_sheet = pd.read_excel(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpvW59A_4xIU"},"outputs":[],"source":["# Remove all rows that do not hold any data\n","patients_sheet = patients_sheet[patients_sheet['Age'].notna()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5JKB0ZV6ZGd"},"outputs":[],"source":["num_of_patients_recorded = len(patients_sheet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDVXftsluSFs"},"outputs":[],"source":["all_patients_diagnosis_col = [-1] * (num_of_patients_recorded+1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IV660wnSsSJk"},"outputs":[],"source":["# Create a bin list of known diseases\n","list_of_diseases = []\n","for index, row in patients_sheet.iterrows():\n","  print(\"INDEX+1:\",index+1,\" diag: \",row['dysphonia diagnosis'])\n","  diagnosis = row['dysphonia diagnosis']\n","  if type(diagnosis) == str:\n","    for word in diagnosis.split(\"+\"):\n","      stripped = word.strip()\n","      if stripped not in list_of_diseases:\n","        list_of_diseases.append(stripped)\n","\n","for index, row in patients_sheet.iterrows():\n","  diagnosis = row['dysphonia diagnosis']\n","  if type(diagnosis) == str:\n","    diagnosis_list = row['dysphonia diagnosis'].split(\"+\")\n","    stripped_list = []\n","    for diagnosis in diagnosis_list:\n","      stripped_list.append(diagnosis.strip())\n","      all_patients_diagnosis_col[index+1] = stripped_list"]},{"cell_type":"markdown","metadata":{"id":"RudTe4cDdpkD"},"source":["Our \"bank\" of seen diseases:"]},{"cell_type":"code","source":["# list_of_diseases"],"metadata":{"id":"AT2iBiki4pWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwbCm6-0dmcG"},"outputs":[],"source":["# all_patients_diagnosis_col"]},{"cell_type":"markdown","metadata":{"id":"D2Tf1zoVIfn5"},"source":["Main loop: Iterate over all patients data in drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLH1w49cmRYM"},"outputs":[],"source":["# create empty list of lists,\n","pat_list = [[None]] * (num_of_patients_recorded+1)\n","# Iterate over all wav files in the drive\n","def iterate_all_pats(phase = 1):\n","  for file in glob.glob(my_voice_data_no_spaces + \"/*.wav\"):\n","      pat_num_str = re.findall(r'\\d+', file)[0]\n","      print(f'------------ Working on patient {pat_num_str} -------------- ')\n","      if( int(pat_num_str) <= num_of_patients_recorded):\n","        if(phase == 1):\n","          pat_list[int(pat_num_str)] = audio_split_phase1(pat_num_str)\n","        if(phase == 2):\n","          pat_list[int(pat_num_str)] = audio_split_phase2(pat_num_str,save_to_drive=True)\n","      else:\n","        print(\"Patient is not on the excel table\")\n","\n","  # Convert list of lists into pandas df\n","  if(phase == 1):\n","    df = pd.DataFrame(pat_list, columns = audio_labels)\n","  if(phase == 2):\n","    df = pd.DataFrame(pat_list, columns = [\"Audio\"])\n","  # Adds diagnosis columns from excel sheet\n","  df['Diagnosis'] = all_patients_diagnosis_col\n","  return df\n","\n","# choose\n","def create_audio_df(phase = 0):\n","  if(phase==0):\n","    print(\"No phase chosen, 1 for vowel chunks, 2 for full recordings minus the noise\")\n","  else:\n","    df = iterate_all_pats(phase = phase)\n","    # create df file name with path\n","    today = date.today()\n","    print(today)\n","    csv_save_path = '/content/drive/MyDrive/Study_materials/Voice disorder detection project/patients_df_'+ str(today)\n","    # Save the file\n","    dill.dump(df, file = open(csv_save_path+\".pickle\", \"wb\"))\n","    return df\n"]},{"cell_type":"code","source":["df_created = create_audio_df(phase = 2)"],"metadata":{"id":"Tq3w2nr85os_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjoMkJLjIEUZ"},"outputs":[],"source":["# Reload the file\n","load = '/content/drive/MyDrive/Study_materials/Voice disorder detection project/patients_df_2024-04-21.pickle'\n","df_reloaded = dill.load(open(load, \"rb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jFp6Jk8IZ4R"},"outputs":[],"source":["df_reloaded[235:245]"]},{"cell_type":"markdown","metadata":{"id":"QXaVvjf-XZt6"},"source":["Example on how to access data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1712159589950,"user":{"displayName":"Or Beyar","userId":"10215772442836299100"},"user_tz":-180},"id":"EsXPsW87XZcI","outputId":"501233e6-0f44-4d99-88af-b1028e75f8f0"},"outputs":[{"data":{"text/plain":["array([-367, -303, -226, ..., -180, -157, -133], dtype=int16)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Accessing patient 2, index 0 recording ( low pitch E)\n","\n","# patient_num - Patient's number\n","# chunk_num - patient's audio chunk number (chronological)\n","# sample_rate - chunk's sample rate\n","# date - The chunk's recording data - np array\n","df_reloaded.iloc[127,0].data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Owb3uuOJ-AyY"},"outputs":[],"source":["a = df_reloaded.iloc[127,9 ]\n","Audio(a.data, rate=16000)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPLbBHh9J7ysicfMERqouoo"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}