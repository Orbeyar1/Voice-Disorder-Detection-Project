{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2875,"status":"ok","timestamp":1716814373262,"user":{"displayName":"Or Beyar","userId":"10215772442836299100"},"user_tz":-180},"id":"ZwLBjg69FjeM","outputId":"84fb8206-4750-450f-c835-2b7d3a4446ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import sys\n","drive.mount('/content/drive/',force_remount = False)\n","pat_data_sheet_path = \"/content/drive/MyDrive/Study_materials/Voice disorder detection project/data/Raw_data/CRF table.xlsx\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AeapMOu7FhqU"},"outputs":[],"source":["from random import sample\n","import torch\n","from torch.nn import functional as F\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","import torchaudio.transforms as ta_trans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DG6D16wlFc-B"},"outputs":[],"source":["class ToOneHot(nn.Module):\n","    def __call__(self, classification):\n","        if not isinstance(classification,bool):\n","            classification = torch.Tensor([classification]).to(torch.int64)\n","            classification = F.one_hot(classification,10).squeeze()\n","        return classification\n","class CFloat(nn.Module):\n","    def __call__(self,data):\n","        # print(data.dtype)\n","        data = data.cfloat()\n","        return data\n","class ToTensor(nn.Module):\n","    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n","    def __init__(self):\n","        self.device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\n","\n","\n","    def __call__(self, sample):\n","        return torch.from_numpy(sample).float()\n","\n","class Inflate(nn.Module):\n","    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n","\n","    def __call__(self, sample):\n","        return sample.reshape(1,1,len(sample))\n","\n","class Deflate(nn.Module):\n","    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n","\n","    def __call__(self, sample):\n","        return sample.reshape(sample.shape[2])\n","\n","\n","class PadWhiteNoise(nn.Module):\n","    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n","\n","    def __init__(self,length):\n","        self.length = length\n","\n","    def __call__(self,sample,sr=50000):\n","        if len(sample)>self.length:\n","            return sample\n","        mean = sample.mean()\n","        variance = sample.var()\n","        noise = (torch.normal(mean.item(),variance.item(),size=(self.length-len(sample),),device=sample.device))/1200000\n","        signal=torch.cat((sample,noise)).to(device=sample.device)\n","\n","        return signal\n","\n","\n","class Truncate(nn.Module):\n","    def __init__(self,N):\n","        self.N=int(N)\n","    def __call__(self, sample):\n","        return sample[:self.N].reshape(1,-1)\n","\n","class WaveformToInput(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        audio_sample_rate = CommonParams.TARGET_SAMPLE_RATE\n","        window_length_samples = int(round(\n","            audio_sample_rate * CommonParams.STFT_WINDOW_LENGTH_SECONDS\n","        ))\n","        hop_length_samples = int(round(\n","            audio_sample_rate * CommonParams.STFT_HOP_LENGTH_SECONDS\n","        ))\n","        fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n","        assert window_length_samples == 400\n","        assert hop_length_samples == 160\n","        assert fft_length == 512\n","        self.mel_trans_ope = VGGishLogMelSpectrogram(\n","            CommonParams.TARGET_SAMPLE_RATE, n_fft=fft_length,\n","            win_length=window_length_samples, hop_length=hop_length_samples,\n","            f_min=CommonParams.MEL_MIN_HZ,\n","            f_max=CommonParams.MEL_MAX_HZ,\n","            n_mels=CommonParams.NUM_MEL_BANDS\n","        )\n","        # note that the STFT filtering logic is exactly the same as that of a\n","        # conv kernel. It is the center of the kernel, not the left edge of the\n","        # kernel that is aligned at the start of the signal.\n","\n","    #TODO change hard coded number to configuration\n","    def __call__(self, waveform):\n","        res = self.wavform_to_log_mel(waveform=waveform,sample_rate=CommonParams.SVD_SAMPLE_RATE)[0]\n","        shape = res.shape\n","        return res[0].reshape(*shape[1:])\n","    #     '''\n","    #     Args:\n","    #         waveform: torch tsr [num_audio_channels, num_time_steps]\n","    #         sample_rate: per second sample rate\n","    #     Returns:\n","    #         batched torch tsr of shape [N, C, T]\n","    #     '''\n","    #     x = waveform.mean(axis=0, keepdims=True)  # average over channels\n","    #     resampler = ta_trans.Resample(sample_rate, CommonParams.TARGET_SAMPLE_RATE)\n","    #     x = resampler(x)\n","    #     x = self.mel_trans_ope(x)\n","    #     x = x.squeeze(dim=0).T  # # [1, C, T] -> [T, C]\n","\n","    #     window_size_in_frames = int(round(\n","    #         CommonParams.PATCH_WINDOW_IN_SECONDS / CommonParams.STFT_HOP_LENGTH_SECONDS\n","    #     ))\n","    #     print(CommonParams.PATCH_WINDOW_IN_SECONDS)\n","\n","    #     num_chunks = x.shape[0] // window_size_in_frames\n","\n","    #     # reshape into chunks of non-overlapping sliding window\n","    #     num_frames_to_use = num_chunks * window_size_in_frames\n","    #     x = x[:num_frames_to_use]\n","    #     # [num_chunks, 1, window_size, num_freq]\n","    #     x = x.reshape(num_chunks, 1, window_size_in_frames, x.shape[-1])\n","    #     return x\n","\n","    def wavform_to_log_mel(self, waveform, sample_rate):\n","        '''\n","        Args:\n","            waveform: torch tsr [num_audio_channels, num_time_steps]\n","            sample_rate: per second sample rate\n","        Returns:\n","            batched torch tsr of shape [N, C, T]\n","        '''\n","        x = waveform.mean(axis=0, keepdims=True)  # average over channels\n","\n","        resampler = ta_trans.Resample(sample_rate, CommonParams.TARGET_SAMPLE_RATE)\n","        x = resampler(x)\n","        x = self.mel_trans_ope(x)\n","        x = x.squeeze(dim=0).T  # # [1, C, T] -> [T, C]\n","        spectrogram = x.cpu().numpy().copy()\n","\n","        window_size_in_frames = int(round(\n","            CommonParams.PATCH_WINDOW_IN_SECONDS / CommonParams.STFT_HOP_LENGTH_SECONDS\n","        ))\n","\n","        if YAMNetParams.PATCH_HOP_SECONDS == YAMNetParams.PATCH_WINDOW_SECONDS:\n","            num_chunks = x.shape[0] // window_size_in_frames\n","\n","            # reshape into chunks of non-overlapping sliding window\n","            num_frames_to_use = num_chunks * window_size_in_frames\n","            x = x[:num_frames_to_use]\n","            # [num_chunks, 1, window_size, num_freq]\n","            x = x.reshape(num_chunks, 1, window_size_in_frames, x.shape[-1])\n","        else:  # generate chunks with custom sliding window length `patch_hop_seconds`\n","            patch_hop_in_frames = int(round(\n","                YAMNetParams.PATCH_HOP_SECONDS / CommonParams.STFT_HOP_LENGTH_SECONDS\n","            ))\n","            # TODO performance optimization with zero copy\n","            patch_hop_num_chunks = (x.shape[0] - window_size_in_frames) // patch_hop_in_frames + 1\n","            num_frames_to_use = window_size_in_frames + (patch_hop_num_chunks - 1) * patch_hop_in_frames\n","            x = x[:num_frames_to_use]\n","            x = x.reshape(1,1,-1, x.shape[-1])\n","        return x, spectrogram\n","\n","class VGGishLogMelSpectrogram(ta_trans.MelSpectrogram):\n","    '''\n","    This is a _log_ mel-spectrogram transform that adheres to the transform\n","    used by Google's vggish model input processing pipeline\n","    '''\n","\n","    def forward(self, waveform):\n","        r\"\"\"\n","        Args:\n","            waveform (torch.Tensor): Tensor of audio of dimension (..., time)\n","\n","        Returns:\n","            torch.Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time)\n","        \"\"\"\n","        specgram = self.spectrogram(waveform)\n","        # NOTE at mel_features.py:98, googlers used np.abs on fft output and\n","        # as a result, the output is just the norm of spectrogram raised to power 1\n","        # For torchaudio.MelSpectrogram, however, the default\n","        # power for its spectrogram is 2.0. Hence we need to sqrt it.\n","        # I can change the power arg at the constructor level, but I don't\n","        # want to make the code too dirty\n","        specgram = specgram ** 0.5\n","\n","        mel_specgram = self.mel_scale(specgram)\n","        mel_specgram = torch.log(mel_specgram + CommonParams.LOG_OFFSET)\n","        return mel_specgram\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNv4rfRgPYhfzEJOPU+pvj7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}